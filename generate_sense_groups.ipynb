{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sense Vectors to identify Word sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "import plotly.graph_objects as go\n",
    "from plotly.io import write_image, write_html\n",
    "import igraph\n",
    "from igraph import Graph, EdgeSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the github repo to system path, and import modules from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell 2\n",
    "sys.path.append(\"sensegram_package/\")\n",
    "import sensegram\n",
    "from wsd import WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "load_vectors = True # Flag to decide to load sense and word vectors, to word with WSD module\n",
    "load_data = True # Flag to decide to load raw data\n",
    "generate_sense_embeddings = False # Flag to decide to use vectors to generate sense embeddings for each noun in corpus\n",
    "generate_hypernymy_flag = True # Flag to decide to use wordnet to generate hypernymy map for each noun in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file paths\n",
    "data_directory = os.path.join(os.getcwd(), \"data\")\n",
    "outputs_directory = os.path.join(os.getcwd(), \"output\")\n",
    "\n",
    "corpus_fpath = os.path.join(data_directory, \"0.txt\")\n",
    "sense_vectors_fpath = os.path.join(data_directory, \"model\", \"0.sense_vectors\")\n",
    "# word_vectors_fpath = os.path.join(data_directory, \"model\", \"word2vec_twitter_tokens.word_vectors.cbow1-size300-window5-iter5-mincount10-bigramsFalse.word_vectors\")\n",
    "word_vectors_fpath = os.path.join(data_directory, \"model\", \"0.word_vectors\")\n",
    "corpus_data_sense_mappings_file = os.path.join(data_directory, \"corpus_sense_mapping.csv\")\n",
    "hhm_mappings_file = os.path.join(data_directory, \"hhm_mappings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sense and word vector files. This may take some time, owing to the large file size of the vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 10.56437087059021seconds to load vector files\n",
      "Reading the corpus now!\n"
     ]
    }
   ],
   "source": [
    "# Vector & corpus loader\n",
    "if load_vectors:\n",
    "    s = time.time()\n",
    "\n",
    "    if os.path.exists(sense_vectors_fpath) and os.path.exists(word_vectors_fpath):\n",
    "        sense_vectors = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)\n",
    "        word_vectors = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "        print(f\"Took {time.time()-s}seconds to load vector files\")\n",
    "    else:\n",
    "        print(\"Could not find vector files. Check file paths and ensure the right files exists\")\n",
    "    del s\n",
    "\n",
    "if load_data:\n",
    "    print(\"Reading the corpus now!\")\n",
    "    with open(corpus_fpath, \"r\") as f:\n",
    "        corpus_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all senses of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sense vectors, load all possible senses for the given word.  \n",
    "The output prints the sense *Word#&lt;sense-number&gt;* followed by the probabilities of that word matching other words with similar sense. This table can help us provide logical names of different sense groups. For example, running the code for the word \"**table**\" gives the following output -  \n",
    "```\n",
    "Probabilities of the senses:\n",
    "[('Table#1', 1.0), ('Table#2', 1.0), ('Table#3', 1.0), ('Table#4', 1.0), ('table#1', 1.0), ('table#2', 1.0), ('table#3', 1.0), ('table#4', 1.0)]\n",
    "\n",
    "\n",
    "Table#1\n",
    "====================\n",
    "table#1 0.996316\n",
    "TABLE#1 0.993647\n",
    "PAGE#2 0.989991\n",
    "page#2 0.989991\n",
    "WINDOW#2 0.989900\n",
    "Window#3 0.989900\n",
    "window#2 0.989900\n",
    "Scale#2 0.989745\n",
    "scale#2 0.989745\n",
    "SCALE#2 0.989745\n",
    "\n",
    "\n",
    "Table#2\n",
    "====================\n",
    "TABLE#2 1.000000\n",
    "Row#3 0.869726\n",
    "row#3 0.869726\n",
    "ROW#3 0.856643\n",
    "Stack#3 0.829349\n",
    "Box#3 0.826571\n",
    "BOX#2 0.826571\n",
    "stack#3 0.825068\n",
    "STACK#3 0.824239\n",
    "BOWL#3 0.813412\n",
    "\n",
    "\n",
    "Table#3\n",
    "====================\n",
    "TABLE#3 0.939938\n",
    "table#3 0.934190\n",
    "Boundary_Markers#5 0.845184\n",
    "Catchment_Basins#2 0.826906\n",
    "contents#2 0.825448\n",
    "CONTENTS#2 0.825448\n",
    "Contents#2 0.824324\n",
    "tables#1 0.806271\n",
    "NUMBERS#3 0.804637\n",
    "Tables#1 0.796628\n",
    "....\n",
    "\n",
    "```\n",
    "Few things we can see from the output - \n",
    "* Since we have set *ignore_case=True*, the output shows 4 senses for *Table*, and 4 for *table*.\n",
    "* Looking at the related words for each sense, we can attribute the following logical groups to few of the senses - \n",
    "    - Table#2 - Data table.  \n",
    "    - Table#3 - Table of contents.\n",
    "    - table#4 - Hotel/Furniture.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sense extraction helper function\n",
    "def print_word_senses(word, corpus_data, sense_vectors, ignore_case=False, only_in_corpus=False):\n",
    "    senses = sense_vectors.get_senses(word, ignore_case=ignore_case)\n",
    "    \n",
    "    print(\"Probabilities of the senses:\\n{}\\n\\n\".format(senses))\n",
    "    for sense_id, prob in senses:\n",
    "        print(sense_id)\n",
    "        print(\"=\"*20)\n",
    "        for rsense_id, sim in sense_vectors.wv.most_similar(sense_id):\n",
    "            sense_word = rsense_id.split(\"#\")[0]\n",
    "            sense_word = sense_word if ignore_case else sense_word.lower()\n",
    "            count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(sense_word), corpus_data))\n",
    "            if only_in_corpus and count==0:\n",
    "                continue\n",
    "            print(f\"{rsense_id} {round(sim,4)} [{count}]\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of the senses:\n",
      "[('chat#1', 1.0), ('chat#2', 1.0), ('chat#3', 1.0), ('chat#4', 1.0)]\n",
      "\n",
      "\n",
      "chat#1\n",
      "====================\n",
      "lesson#1 0.873 [123]\n",
      "session#1 0.8582 [341]\n",
      "presentation#1 0.846 [190]\n",
      "lecture#1 0.8419 [105]\n",
      "meeting#1 0.8344 [863]\n",
      "workshop#1 0.8329 [113]\n",
      "event#1 0.8145 [533]\n",
      "discussion#1 0.8101 [172]\n",
      "interview#1 0.8054 [658]\n",
      "conversation#1 0.7903 [271]\n",
      "\n",
      "\n",
      "chat#2\n",
      "====================\n",
      "link#1 0.9341 [1197]\n",
      "message#1 0.9073 [459]\n",
      "thanks#2 0.907 [11141]\n",
      "reply#1 0.9064 [459]\n",
      "me#2 0.8931 [23831]\n",
      "emails#1 0.8743 [308]\n",
      "email#1 0.8723 [1610]\n",
      "tweet#1 0.8705 [2553]\n",
      "comment#1 0.8681 [584]\n",
      "someone#4 0.8618 [2245]\n",
      "\n",
      "\n",
      "chat#3\n",
      "====================\n",
      "begin#1 0.9075 [156]\n",
      "talk#1 0.8921 [1253]\n",
      "meet#1 0.8913 [999]\n",
      "forward#2 0.8897 [1527]\n",
      "deal#2 0.8754 [496]\n",
      "listen#1 0.8742 [747]\n",
      "start#1 0.8595 [1684]\n",
      "celebrate#2 0.8588 [166]\n",
      "visit#1 0.8474 [538]\n",
      "see#1 0.8465 [8681]\n",
      "\n",
      "\n",
      "chat#4\n",
      "====================\n",
      "talk#2 0.9712 [1253]\n",
      "talked#2 0.9453 [114]\n",
      "ranting#1 0.9271 [37]\n",
      "rant#1 0.9125 [94]\n",
      "talking#1 0.8902 [1129]\n",
      "hearing#1 0.8849 [262]\n",
      "moan#2 0.8848 [44]\n",
      "fuss#1 0.8806 [35]\n",
      "complained#1 0.856 [22]\n",
      "talks#1 0.8515 [191]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display senses of word\n",
    "if load_vectors:\n",
    "    test_word = \"chat\"\n",
    "    print_word_senses(test_word, corpus_data, sense_vectors, ignore_case=True, only_in_corpus=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get disambiguated sense of the word, using corpus as context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input\n",
    "To understand the word's sense in a given context, we use the *WSD* class from the sensegram library.  \n",
    "The WSD model takes the following key parameters to decide word sense based on corpus context - \n",
    "* vectors - Both sense and word vector models loaded earlier.  \n",
    "  \n",
    "  \n",
    "* method - To calculate the sense of the word, the library averages the sense scores of all the surrounding context words and compares it with different senses of the target word. For this comparison, there are two available metrics - \n",
    " - sim: Uses cosine distance\n",
    " - prob: Use log probability score  \n",
    "  \n",
    "  \n",
    "* window - This is the window(±) that the model looks into, to decide the word context.   \n",
    "For example, if our target word is *table*,   \n",
    "with the context of *\"we load the our data into a data-frame table object and count the number of rows/columns using the .shape method\"*  \n",
    " 1. a window of 3 would consider the following 6(3 on the left, and 3 on the right) words around our context word to find the sense of the word - *into, a, data-frame, object, and, count*  \n",
    " 2. a window of 5 would use the following context - *our, data, into, a, data-frame, table, object, and, count, the, number*  \n",
    "  \n",
    "  \n",
    "* verbose - Allows to print intermediate outputs while running the disambiguation code\n",
    "\n",
    "<hr>  \n",
    "     \n",
    "Some food-for-thought regarding the usage of WSD module - \n",
    " - Do note that while stopwords like *and* and *the* are considered in the context of the the target word, they are dropped while disambiguating the sense of our target.\n",
    " - While it may seem ideal to choose a high value of window for getting the sense of the target word, it may happen that the wider window results in an less accurate output, as it averages across all possible senses.\n",
    " - The library considers, and disambiguates, only the first occurance of the target word in the context. For a large corpus, it would be ideal to first split the corpus and generate contexts using an external helper function, and then iteratively get the sense for the target word across all occurances in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Word Sense Disambiguation model\n",
    "if load_vectors:\n",
    "    wsd_model = WSD(sense_vectors, word_vectors, window=15, method='prob', verbose=True)\n",
    "else:\n",
    "    print(\"Load vectors to initialize and work with WSD model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted context words:\n",
      "['sell', 'yes', 'Im', 'bath', 'tell', 'haha', 'amazing', 'please', 'shes', 'list', 'captured', 'pants', 'uk', 'cctv']\n",
      "Senses of a target word:\n",
      "[('morning#1', 1.0)]\n",
      "Significance scores of context words:\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Context words:\n",
      "sell\t0.000\n",
      "yes\t0.000\n",
      "Im\t0.000\n",
      "('morning#1', [0.6330168541350161])\n"
     ]
    }
   ],
   "source": [
    "# Extracting disambiguates senses\n",
    "if load_vectors:\n",
    "    print(wsd_model.disambiguate(corpus_data, \"morning\"))\n",
    "else:\n",
    "    print(\"Load vectors to initialize and work with WSD model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Output </h5>  \n",
    "\n",
    "Running the Sense disambiguation code generates following lines of output -  \n",
    "1. Prints the context words extracted from the corpus.\n",
    "- Prints possible senses of the word, with their respective probabilities(without considering the context)\n",
    "- Prints the significance score of each context word.\n",
    "- Prints the most significant context words.\n",
    "- **Returns** a tuple of the sense of the word as derived from the context, and match scores(log-probability or cosine-similarity depending on the *method* chosen) of various senses of the target word.  \n",
    "For instance, the output *('table#2', [0.2706353009709064, 0.9591583572384959, 0.40617065436041355, 0.6940131864117054])* indicates the following things regarding our target word -  \n",
    "    - The closest sense of our target word is with *table#2*, with a match score of 0.959(second in the list)\n",
    "    - For the other senses, the match score can be read as follows - \n",
    "    - table#1 - 0.2706\n",
    "    - table#2 - 0.959\n",
    "    - table#3 - 0.406\n",
    "    - table#4 - 0.694\n",
    "\n",
    "Since we had not defined the *ignore_case* argument while initializing the WSD model, it resorts to the default of True, and the output return scores for the 4 senses of the word *table*.  \n",
    "If we chose to ignore case, the output would have match for 8 senses(4-Table; 4-table)\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.count(\"morning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sense embeddings from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_data_with_context(corpus_filepath, window=5, force_update=False):\n",
    "    nlp = spacy.load('en')\n",
    "    with open(corpus_filepath, \"r\") as f:\n",
    "        corpus_data = f.readlines()\n",
    "    out_file = os.path.join(os.path.dirname(corpus_filepath), f\"window_{window}_context_corpus.txt\")\n",
    "    contextual_data = []\n",
    "    if os.path.exists(out_file) and (not force_update):\n",
    "        print(\"Found preexisting file. Loading context data from file\")\n",
    "        with open(out_file, \"r\") as f:\n",
    "            contextual_data = f.readlines()\n",
    "    else:\n",
    "        print(\"No pre created corpus found, or force update flag is true. Generating contextual data from corpus\")\n",
    "        with open(out_file, \"w\") as f:\n",
    "            for txt_line in corpus_data:\n",
    "                nlp_data = nlp(txt_line.replace(\"\\n\", \"\").replace(\"-\", \"\"))\n",
    "                max_tokens = len(nlp_data)\n",
    "                for i, tok in enumerate(nlp_data):\n",
    "                    if \"NN\" in tok.tag_:\n",
    "                        start = max(0, i-window)\n",
    "                        end = min(i+window, max_tokens)\n",
    "                        left_context = [t.text for t in nlp_data[start:i]] + [f\"<{tok.text}>\"]\n",
    "                        right_context = [t.text for t in nlp_data[i+1:end]]\n",
    "                        noun_in_context = f\"<{tok.text}> - {' '.join(left_context + right_context)}\"\n",
    "                        contextual_data.append(noun_in_context)\n",
    "                        f.write(noun_in_context + \"\\n\")\n",
    "    return contextual_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_group_from_corpus(context_data, wsd_model, sense_vectors):\n",
    "    output = {\n",
    "        \"word\": [],\n",
    "        \"context\": [],\n",
    "        \"sense_id\": [],\n",
    "        \"sense_group_name\": [],\n",
    "        \"sense_group_num\": [],\n",
    "        \"sense_probability\": [],\n",
    "        \"related_senses\": []\n",
    "    }\n",
    "    \n",
    "    for row in tqdm(context_data, total=len(context_data)):\n",
    "        word, ctx = row.split(' - ')\n",
    "        sense_id, sense_probs = wsd_model.disambiguate(ctx, word)\n",
    "        sense_probability = max(sense_probs)\n",
    "        sense_group_name, sense_group_num = sense_id.split(\"#\")\n",
    "        try:\n",
    "            related_senses = [r_senseid for r_senseid,_ in sense_vectors.wv.most_similar(sense_id)]\n",
    "            related_senses_l2 = [r_senseid for related_sense in related_senses for r_senseid,_ in sense_vectors.wv.most_similar(related_sense)]\n",
    "        except KeyError as e:\n",
    "            print(f\"Could not get related senses for {sense_id}\")\n",
    "            related_senses = []\n",
    "            related_senses_l2 = []\n",
    "        output[\"word\"].append(word)\n",
    "        output[\"context\"].append(ctx)\n",
    "        output[\"sense_id\"].append(sense_id)\n",
    "        output[\"sense_group_name\"].append(sense_group_name)\n",
    "        output[\"sense_group_num\"].append(sense_group_num)\n",
    "        output[\"sense_probability\"].append(sense_probability)\n",
    "        output[\"related_senses\"].append(related_senses+related_senses_l2)\n",
    "    \n",
    "    output_df = pd.DataFrame(output)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_func(p_name, tasks, results):\n",
    "    while True:\n",
    "        indices = tasks.get()\n",
    "        if indices == None:\n",
    "            print('[%s] evaluation routine quits' % p_name)\n",
    "\n",
    "            # Indicate finished\n",
    "            results.put(\"Done\")\n",
    "            break\n",
    "        else:\n",
    "            # Compute result and mimic a long-running task\n",
    "            batch_df = get_sense_group_from_corpus(contextual_data[indices[0]:indices[1]], wsd_model2, sense_vectors)\n",
    "\n",
    "            # Output which process received the value\n",
    "            # and the calculation result\n",
    "            print(f\"{p_name} recieved computation of batch {indices}\")\n",
    "\n",
    "            # Add result to the queue\n",
    "            results.put(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use WSD model to create file with Sense mappings\n",
    "if (not os.path.exists(corpus_data_sense_mappings_file)) or generate_sense_embeddings:\n",
    "    manager = multiprocessing.Manager()\n",
    "    tasks = manager.Queue()\n",
    "    results = manager.Queue()\n",
    "    num_processes = os.cpu_count()\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    processes = []\n",
    "    \n",
    "    base_df = pd.DataFrame({\n",
    "        \"word\": [],\n",
    "        \"context\": [],\n",
    "        \"sense_id\": [],\n",
    "        \"sense_group_name\": [],\n",
    "        \"sense_group_num\": [],\n",
    "        \"sense_probability\": [],\n",
    "        \"related_senses\": []\n",
    "    })\n",
    "    wsd_model2 = WSD(sense_vectors, word_vectors, window=window, method='prob', verbose=False)\n",
    "    contextual_data = prepare_corpus_data_with_context(corpus_fpath, window=window, force_update=True)\n",
    "    num_lines = len(contextual_data)\n",
    "    batch_size = int(num_lines/num_processes)+1\n",
    "    \n",
    "    for i in range(num_processes):\n",
    "        proc_name = f\"P_{i}\"\n",
    "        start_idx = i*batch_size\n",
    "        end_idx = min(num_lines, batch_size*(i+1))\n",
    "        tasks.put([start_idx, end_idx])\n",
    "        new_process = multiprocessing.Process(target=worker_func, args=(proc_name,tasks,results))\n",
    "        processes.append(new_process)\n",
    "        new_process.start()\n",
    "        \n",
    "    for i in range(num_processes):\n",
    "        tasks.put(None)\n",
    "    \n",
    "    num_finished_processes = 0\n",
    "    while True: \n",
    "        new_result = results.get()\n",
    "        if new_result == \"Done\":\n",
    "            # Process has finished\n",
    "            num_finished_processes += 1\n",
    "            if num_finished_processes == num_processes:\n",
    "                break\n",
    "        else:\n",
    "            output_df = output_df.append(new_result)\n",
    "\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "else:\n",
    "    sense_group_dataframe = pd.read_csv(corpus_data_sense_mappings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>context</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>sense_group_name</th>\n",
       "      <th>sense_group_num</th>\n",
       "      <th>sense_probability</th>\n",
       "      <th>related_senses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unread&gt;</td>\n",
       "      <td>I did that too think its best &lt;unread&gt;</td>\n",
       "      <td>unread#1</td>\n",
       "      <td>unread</td>\n",
       "      <td>1</td>\n",
       "      <td>0.426093</td>\n",
       "      <td>['exceeded#1', 'renewed#1', 'countless#1', 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;shopping&gt;</td>\n",
       "      <td>&lt;shopping&gt; list for baby penguins and seals co...</td>\n",
       "      <td>shopping#1</td>\n",
       "      <td>shopping</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752219</td>\n",
       "      <td>['gym#1', 'xmas#1', 'finish#2', 'drinks#1', 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;list&gt;</td>\n",
       "      <td>shopping &lt;list&gt; for baby penguins and seals co...</td>\n",
       "      <td>list#1</td>\n",
       "      <td>list</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563295</td>\n",
       "      <td>['website#1', 'tumblr#1', 'page#1', 'facebook#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;baby&gt;</td>\n",
       "      <td>shopping list for &lt;baby&gt; penguins and seals co...</td>\n",
       "      <td>baby#1</td>\n",
       "      <td>baby</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782509</td>\n",
       "      <td>['brother#1', 'yo#2', 'mummy#1', 'hubby#1', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;penguins&gt;</td>\n",
       "      <td>shopping list for baby &lt;penguins&gt; and seals co...</td>\n",
       "      <td>penguins#1</td>\n",
       "      <td>penguins</td>\n",
       "      <td>1</td>\n",
       "      <td>0.802287</td>\n",
       "      <td>['ducks#1', 'scones#1', 'burgers#1', 'skirts#1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word                                            context    sense_id  \\\n",
       "0    <unread>             I did that too think its best <unread>    unread#1   \n",
       "1  <shopping>  <shopping> list for baby penguins and seals co...  shopping#1   \n",
       "2      <list>  shopping <list> for baby penguins and seals co...      list#1   \n",
       "3      <baby>  shopping list for <baby> penguins and seals co...      baby#1   \n",
       "4  <penguins>  shopping list for baby <penguins> and seals co...  penguins#1   \n",
       "\n",
       "  sense_group_name  sense_group_num  sense_probability  \\\n",
       "0           unread                1           0.426093   \n",
       "1         shopping                1           0.752219   \n",
       "2             list                1           0.563295   \n",
       "3             baby                1           0.782509   \n",
       "4         penguins                1           0.802287   \n",
       "\n",
       "                                      related_senses  \n",
       "0  ['exceeded#1', 'renewed#1', 'countless#1', 're...  \n",
       "1  ['gym#1', 'xmas#1', 'finish#2', 'drinks#1', 'p...  \n",
       "2  ['website#1', 'tumblr#1', 'page#1', 'facebook#...  \n",
       "3  ['brother#1', 'yo#2', 'mummy#1', 'hubby#1', 'b...  \n",
       "4  ['ducks#1', 'scones#1', 'burgers#1', 'skirts#1...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_group_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernymy extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for creating HHM map\n",
    "weights = {\n",
    "    \"direct_match\": 8,\n",
    "    \"direct_nomatch\": 5,\n",
    "    \"l1_match\": 3,\n",
    "    \"l1_nomatch\": 2,\n",
    "    \"l2_match\": 1,\n",
    "    \"l2_nomatch\": 0.5,\n",
    "}\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypernymy(word, idx, hypernymy_dict, level=\"l1\"):\n",
    "    token = nlp(str(word))[0]\n",
    "    synsets = token._.wordnet.synsets()\n",
    "    for syn in synsets:\n",
    "        if syn.name().split(\".n.\")[0] == word and int(syn.name().split(\".n.\")[1]) == idx:\n",
    "            match_str = \"match\"\n",
    "        else:\n",
    "            match_str = \"nomatch\"\n",
    "\n",
    "        hypernym_syn = syn.hypernyms()\n",
    "        hyponym_syn = syn.hyponyms()\n",
    "        meronym_syn = syn.part_meronyms()\n",
    "        if len(hypernym_syn) >0:\n",
    "            hypernym = hypernym_syn[0].name().split('.')[0]\n",
    "            rev_map = {\n",
    "#                 f\"{word}#{idx}\":syn,\n",
    "                \"hyponyms\": [hyp.name().split(\".\")[0] for hyp in hyponym_syn],\n",
    "                \"meronyms\": [mero.name().split(\".\")[0] for mero in meronym_syn]\n",
    "            }\n",
    "            \n",
    "            if level in hypernymy_dict[hypernym][\"rev_map\"]:\n",
    "                hypernymy_dict[hypernym][\"rev_map\"][level].append(rev_map) \n",
    "            else:\n",
    "                hypernymy_dict[hypernym][\"rev_map\"][level] = [rev_map]\n",
    "\n",
    "            if \"sum_weight\" in hypernymy_dict[hypernym]:\n",
    "                hypernymy_dict[hypernym][\"sum_weight\"] += weights[f\"{level}_{match_str}\"]\n",
    "            else:\n",
    "                hypernymy_dict[hypernym][\"sum_weight\"] = weights[f\"{level}_{match_str}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyponyms_meronym(hypernymy_dict, hypernym, kind=\"hyponyms\"):\n",
    "    nym = []\n",
    "    reverse_map = hypernymy_dict[hypernym]['rev_map']\n",
    "    extract_from = []\n",
    "    if \"direct\" in reverse_map:\n",
    "        extract_from.append(\"direct\")\n",
    "    if \"l1\" in reverse_map:\n",
    "        extract_from.append(\"l1\")\n",
    "    \n",
    "    for relation in extract_from:\n",
    "        num_relations = len(reverse_map[relation])\n",
    "        for i in range(num_relations):\n",
    "            nym += reverse_map[relation][i][kind]\n",
    "            \n",
    "    return list(set(nym))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17666/1043372 [14:49:33<1320:50:50,  4.64s/it]"
     ]
    }
   ],
   "source": [
    "# Generate HHM flag\n",
    "if generate_hypernymy_flag:\n",
    "    all_hypernyms = []\n",
    "    all_hhm_map = []\n",
    "    all_hyponyms = []\n",
    "    all_meronyms = []\n",
    "\n",
    "    all_hypernyms_from_corpus = []\n",
    "    all_hyponyms_from_corpus = []\n",
    "    all_meronyms_from_corpus = []\n",
    "    num_words = len(sense_group_dataframe)\n",
    "    for _, row in tqdm(sense_group_dataframe[[\"sense_group_name\", \"sense_group_num\", \"related_senses\"]].iterrows(), total=num_words):\n",
    "        word, idx, r_senses = row\n",
    "\n",
    "        hypernymy_dict = defaultdict(lambda: defaultdict(dict))\n",
    "        \n",
    "        generate_hypernymy(word, idx, hypernymy_dict, level=\"direct\")\n",
    "        if type(r_senses) == str:\n",
    "            r_senses_list = ast.literal_eval(r_senses)\n",
    "        else:\n",
    "            r_senses_list = r_senses\n",
    "        for w in r_senses_list:\n",
    "            related_word, related_idx = str(w).split(\"#\")\n",
    "            if related_word in corpus_data:\n",
    "                if len(related_word.strip())>0:\n",
    "                    generate_hypernymy(related_word, related_idx, hypernymy_dict)\n",
    "        hypernyms = []\n",
    "        hyponyms = []\n",
    "        meronyms = []   \n",
    "        hypernyms_from_corpus = []\n",
    "        hyponyms_from_corpus = []\n",
    "        meronyms_from_corpus = []\n",
    "        hypernymy_dict_sorted = {}\n",
    "        if len(hypernymy_dict)>0:\n",
    "            hypernymy_dict_sorted = {k: dict(v) for k, v in sorted(hypernymy_dict.items(), key=lambda item: item[1]['sum_weight'], reverse=True)}\n",
    "\n",
    "            hypernyms = list(hypernymy_dict_sorted.keys())\n",
    "            for hypernym in hypernyms:\n",
    "                extracted_hyponyms = extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym, kind=\"hyponyms\")\n",
    "                extracted_meronyms = extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym, kind=\"meronyms\")\n",
    "                hyponyms.append(extracted_hyponyms)\n",
    "                meronyms.append(extracted_meronyms)\n",
    "                \n",
    "                if hypernym.replace(\"_\", \" \") in corpus_data:\n",
    "                    hypernyms_from_corpus.append(hypernym)\n",
    "                    extracted_hyponyms_from_corpus = [hypo for hypo in extracted_hyponyms if hypo.replace(\"_\", \" \") in corpus_data]\n",
    "                    extracted_meronyms_from_corpus = [mero for mero in extracted_meronyms if mero.replace(\"_\", \" \") in corpus_data]\n",
    "                    hyponyms_from_corpus.append(extracted_hyponyms)\n",
    "                    meronyms_from_corpus.append(extracted_meronyms)\n",
    "\n",
    "        all_hypernyms.append(hypernyms)\n",
    "        all_hyponyms.append(hyponyms)\n",
    "        all_meronyms.append(meronyms)\n",
    "        all_hypernyms_from_corpus.append(hypernyms_from_corpus)\n",
    "        all_hyponyms_from_corpus.append(hyponyms_from_corpus)\n",
    "        all_meronyms_from_corpus.append(meronyms_from_corpus)\n",
    "        all_hhm_map.append(hypernymy_dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update HHM data with mappings\n",
    "if generate_hypernymy_flag:\n",
    "    sense_group_dataframe[\"hhm_map\"] = all_hhm_map\n",
    "    sense_group_dataframe[\"hypernym\"] = all_hypernyms\n",
    "    sense_group_dataframe[\"hyponym\"] = all_hyponyms\n",
    "    sense_group_dataframe[\"meronym\"] = all_meronyms\n",
    "    sense_group_dataframe[\"hypernym_from_corpus\"] = all_hypernyms_from_corpus\n",
    "    sense_group_dataframe[\"hyponym_from_corpus\"] = all_hyponyms_from_corpus\n",
    "    sense_group_dataframe[\"meronym_from_corpus\"] = all_meronyms_from_corpus\n",
    "    sense_group_dataframe.to_csv(hhm_mappings_file, index=False)\n",
    "else:\n",
    "    print(\"Neither generate hypernymy flag, nor hhm mappings file is present. Set the appropriate flags to proceed further\")\n",
    "    \n",
    "hhm_mappings_df = pd.read_csv(hhm_mappings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hhm_mappings_file = os.path.join(\"hhm_mappings.csv\")\n",
    "# sense_group_dataframe = pd.read_csv(hhm_mappings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhm_mappings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernymy Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions - \n",
    "* extract_hypo_mero_map: Extracts mappings of hypernym-hyponym and meronym for selected word. Set the *only_from_corpus* flag True, to extract and maintain only those words that exist in the corpus.\n",
    "* generate_plot_indices - Generates plot indices for nodes and edges to be used while plotting.\n",
    "* make_annotations - Makes annotations to the plot.\n",
    "* generate_plot - Main function that uses the last 3 helper functions to generate plot for hhm mappings and shows them.\n",
    "\n",
    "**NOTE** - If the plot does not show up in the ouput, restart kernel and clear output, and rerun the cells. This is a known issue of plotly with IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hypo_mero_map(word, hhm_mappings_df, only_from_corpus=True):\n",
    "    subset_df = hhm_mappings_df[hhm_mappings_df[\"sense_group_name\"]==word]\n",
    "    \n",
    "    rows_to_consider = [\"hypernym\", \"hyponym\", \"meronym\"]\n",
    "    if only_from_corpus:\n",
    "        rows_to_consider = [\"hypernym_from_corpus\", \"hyponym_from_corpus\", \"meronym_from_corpus\"]\n",
    "    \n",
    "    hypo_mero_map = {}\n",
    "    for _, row in subset_df[rows_to_consider].iterrows():\n",
    "        hyper_str, hypo_str, mero_str = row\n",
    "        hyper_list = ast.literal_eval(hyper_str)\n",
    "        hypo_list = ast.literal_eval(hypo_str)\n",
    "        mero_list = ast.literal_eval(mero_str)\n",
    "        \n",
    "        for hypernym, hyponyms, meronyms in zip(hyper_list, hypo_list, mero_list):\n",
    "            \n",
    "            if hypernym in hypo_mero_map:\n",
    "                hypo_mero_map[hypernym][\"hyponyms\"] =  list(set(hypo_mero_map[hypernym][\"hyponyms\"] + hyponyms))\n",
    "                hypo_mero_map[hypernym][\"meronyms\"] =  list(set(hypo_mero_map[hypernym][\"meronyms\"] + meronyms))\n",
    "            else:\n",
    "                hypo_mero_map[hypernym] = {}\n",
    "                hypo_mero_map[hypernym][\"hyponyms\"] =  list(set(hyponyms))\n",
    "                hypo_mero_map[hypernym][\"meronyms\"] =  list(set(meronyms))\n",
    "    \n",
    "    if len(hypo_mero_map) == 0:\n",
    "        print(f\"Could not find any hypernyms for {word}. Try changing the only from corpus flag. Or try with another word.\")\n",
    "        sys.exit(0)\n",
    "    return hypo_mero_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot_indices(word, hypo_mero_map, top_n=3):   \n",
    "    max_groups = min(top_n, len(hypo_mero_map))\n",
    "    \n",
    "    hyper_start = 0\n",
    "    word_y = 4\n",
    "    word_x = math.ceil(max_groups/2) if max_groups>1 else 0\n",
    "    hyper_y = 6\n",
    "    hyper_x = 0\n",
    "    hypo_x = 0\n",
    "    hypo_y = 2\n",
    "    mero_x = 0\n",
    "    mero_y = 0\n",
    "    label_x = -2\n",
    "    \n",
    "    position = {\n",
    "        \"Hypernyms ->\": [label_x, hyper_y],\n",
    "        \"Word ->\": [label_x, word_y],\n",
    "        \"Hyponyms ->\": [label_x, hypo_y],\n",
    "        \"Meronyms ->\": [label_x, mero_y],\n",
    "        word: [word_x, word_y]\n",
    "    }\n",
    "    edges = []\n",
    "    \n",
    "    keys = list(hypo_mero_map.keys())[:max_groups]\n",
    "    for hyper in keys:\n",
    "        hypo = hypo_mero_map[hyper]['hyponyms']\n",
    "        mero = hypo_mero_map[hyper]['meronyms']\n",
    "        \n",
    "        for i in range(len(hypo)):\n",
    "            if i!=0 and i%3==0:\n",
    "                hypo.insert(i, \"__placeholder__\")\n",
    "\n",
    "        for i in range(len(mero)):\n",
    "            if i!=0 and i%3==0:\n",
    "                mero.insert(i, \"__placeholder__\")\n",
    "        \n",
    "        if hyper==word:\n",
    "            hyper += \"_hypernym\"\n",
    "        \n",
    "        position[hyper] = [hyper_x, hyper_y]\n",
    "        edges.append((word, hyper))\n",
    "        hyper_x +=2\n",
    "\n",
    "        hypo_group = ', '.join(hypo)\n",
    "        position[hypo_group] = [hypo_x, hypo_y]\n",
    "        edges.append((word, hypo_group))\n",
    "        hypo_x +=2\n",
    "        \n",
    "        if len(mero)>0:\n",
    "            mero_group = ', '.join(mero)\n",
    "            position[mero_group] = [mero_x, mero_y]\n",
    "            edges.append((hypo_group, mero_group))\n",
    "            mero_x +=2\n",
    "        \n",
    "    return position, edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_annotations(pos, text, font_size=10, font_color='rgb(0,0,0)'):\n",
    "    L=len(pos)\n",
    "    if len(text)!=L:\n",
    "        raise ValueError('The lists pos and text must have the same len')\n",
    "    annotations = []\n",
    "    labels = [l.replace(\", __placeholder__,\",\"<br>\") for l in text]\n",
    "    for n, k in enumerate(list(pos.keys())):\n",
    "        annotations.append(\n",
    "            dict(\n",
    "                text=labels[n], # or replace labels with a different list for the text within the circle\n",
    "                x=pos[k][0], y=pos[k][1],\n",
    "                xref='x1', yref='y1',\n",
    "                font=dict(color=font_color, size=font_size),\n",
    "                showarrow=False)\n",
    "        )\n",
    "    return annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(word, fig, save_format, top_n):\n",
    "    if not save_format in [\"svg\", \"pdf\", \"html\", \"png\", \"jpg\"]:\n",
    "        print(f\"Unsupported file format {save_format}. Could not save to file\")\n",
    "        sys.exit(0)\n",
    "    out_dir = os.path.join(outputs_directory, save_format)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    output_filepath = os.path.join(outputs_directory, save_format, f\"{word}.{save_format}\")\n",
    "    \n",
    "    if save_format in [\"svg\", \"pdf\", \"png\", \"jpg\"]:\n",
    "        write_image(fig, output_filepath)\n",
    "    else:\n",
    "        write_html(fig, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(word, hhm_mappings_df, only_in_corpus=False, top_n=3, save_format=\"svg\"):\n",
    "    hypo_mero_map = extract_hypo_mero_map(word, hhm_mappings_df, only_from_corpus=only_in_corpus)\n",
    "    position, edges = generate_plot_indices(word, hypo_mero_map, top_n)\n",
    "    \n",
    "    labels = list(position.keys())\n",
    "    hover_labels = [l.replace(\", __placeholder__\", \"\") for l in labels]\n",
    "    \n",
    "    Xn = [position[k][0] for k in labels]\n",
    "    Yn = [position[k][1] for k in labels]\n",
    "\n",
    "    Xe = []\n",
    "    Ye = []\n",
    "    for edge in edges:\n",
    "        Xe+=[position[edge[0]][0],position[edge[1]][0], None]\n",
    "        Ye+=[position[edge[0]][1],position[edge[1]][1], None]\n",
    "        \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=Xe,\n",
    "                       y=Ye,\n",
    "                       mode='lines',\n",
    "                       line=dict(color='rgb(210,210,210)', width=1),\n",
    "                       hoverinfo='none'\n",
    "                       ))\n",
    "    fig.add_trace(go.Scatter(y=Yn,\n",
    "                      x=Xn,\n",
    "                      mode='markers',\n",
    "                      marker=dict(symbol='circle-dot',\n",
    "                                    size=0.0001,\n",
    "                                    color='#6175c1',    #'#DB4551',\n",
    "                                    line=dict(color='rgb(50,50,50)', width=1)\n",
    "                                    ),\n",
    "                      text=hover_labels,\n",
    "                      hoverinfo='text',\n",
    "                      opacity=0.8\n",
    "                      ))\n",
    "    axis = dict(showline=False, # hide axis line, grid, ticklabels and  title\n",
    "            zeroline=False,\n",
    "            showgrid=False,\n",
    "            showticklabels=False,\n",
    "            )\n",
    "\n",
    "    fig.update_layout(title= f'Hypernym-Hyponym-Meronym Tree - {word}',\n",
    "                  annotations=make_annotations(position, labels),\n",
    "                  font_size=12,\n",
    "                  showlegend=False,\n",
    "                  xaxis=axis,\n",
    "                  yaxis=axis,\n",
    "                  margin=dict(l=40, r=40, b=85, t=100),\n",
    "                  hovermode='closest',\n",
    "                  plot_bgcolor='rgb(248,248,248)'\n",
    "                  )\n",
    "    print(\"Save plot to file\")\n",
    "    save_plot(word, fig, save_format, top_n)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the word to one of your choice and re-run to generate graph for that word.\n",
    "Arguments - \n",
    "* *word* - The word for which HHM mappings are to be extracted.\n",
    "* *hhm_mappings_df* - HHM mappings dataframe created in the previous step.\n",
    "* *only_in_corpus* - Flag to be set if the output graph should only show the words that are there in the corpus. This will limit the hypernym, hyponyms and meronyms shown for the focus word.\n",
    "* *top_n* - Top *N* groups to be plotted. Default 3. If the number of HHM groups are less than top_n, then all those groups will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word = \"slices\"\n",
    "generate_plot(word, hhm_mappings_df, only_in_corpus=False, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
