{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sense Vectors to identify Word sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from gensim.models import KeyedVectors\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from spacy_wordnet.wordnet_annotator import WordnetAnnotator \n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the github repo to system path, and import modules from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"sensegram_package/\")\n",
    "import sensegram\n",
    "from wsd import WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join(os.getcwd(), \"data\")\n",
    "corpus_fpath = os.path.join(data_directory, \"corpus.txt\")\n",
    "sense_vectors_fpath = os.path.join(data_directory, \"model\", \"wiki.txt.clusters.minsize5-1000-sum-score-20.sense_vectors\")\n",
    "word_vectors_fpath = os.path.join(data_directory, \"model\", \"wiki.txt.word_vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sense and word vector files. This may take some time, owing to the large file size of the vector files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1278.5607526302338seconds to load vector files\n",
      "Reading the corpus now!\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "if os.path.exists(sense_vectors_fpath) and os.path.exists(word_vectors_fpath):\n",
    "    sense_vectors = sensegram.SenseGram.load_word2vec_format(sense_vectors_fpath, binary=False)\n",
    "    word_vectors = KeyedVectors.load_word2vec_format(word_vectors_fpath, binary=False, unicode_errors=\"ignore\")\n",
    "    print(f\"Took {time.time()-s}seconds to load vector files\")\n",
    "else:\n",
    "    print(\"Could not find vector files. Check file paths and ensure the right files exists\")\n",
    "del s\n",
    "\n",
    "print(\"Reading the corpus now!\")\n",
    "with open(corpus_fpath, \"r\") as f:\n",
    "    corpus_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all senses of a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sense vectors, load all possible senses for the given word.  \n",
    "The output prints the sense *Word#&lt;sense-number&gt;* followed by the probabilities of that word matching other words with similar sense. This table can help us provide logical names of different sense groups. For example, running the code for the word \"**table**\" gives the following output -  \n",
    "```\n",
    "Probabilities of the senses:\n",
    "[('Table#1', 1.0), ('Table#2', 1.0), ('Table#3', 1.0), ('Table#4', 1.0), ('table#1', 1.0), ('table#2', 1.0), ('table#3', 1.0), ('table#4', 1.0)]\n",
    "\n",
    "\n",
    "Table#1\n",
    "====================\n",
    "table#1 0.996316\n",
    "TABLE#1 0.993647\n",
    "PAGE#2 0.989991\n",
    "page#2 0.989991\n",
    "WINDOW#2 0.989900\n",
    "Window#3 0.989900\n",
    "window#2 0.989900\n",
    "Scale#2 0.989745\n",
    "scale#2 0.989745\n",
    "SCALE#2 0.989745\n",
    "\n",
    "\n",
    "Table#2\n",
    "====================\n",
    "TABLE#2 1.000000\n",
    "Row#3 0.869726\n",
    "row#3 0.869726\n",
    "ROW#3 0.856643\n",
    "Stack#3 0.829349\n",
    "Box#3 0.826571\n",
    "BOX#2 0.826571\n",
    "stack#3 0.825068\n",
    "STACK#3 0.824239\n",
    "BOWL#3 0.813412\n",
    "\n",
    "\n",
    "Table#3\n",
    "====================\n",
    "TABLE#3 0.939938\n",
    "table#3 0.934190\n",
    "Boundary_Markers#5 0.845184\n",
    "Catchment_Basins#2 0.826906\n",
    "contents#2 0.825448\n",
    "CONTENTS#2 0.825448\n",
    "Contents#2 0.824324\n",
    "tables#1 0.806271\n",
    "NUMBERS#3 0.804637\n",
    "Tables#1 0.796628\n",
    "....\n",
    "\n",
    "```\n",
    "Few things we can see from the output - \n",
    "* Since we have set *ignore_case=True*, the output shows 4 senses for *Table*, and 4 for *table*.\n",
    "* Looking at the related words for each sense, we can attribute the following logical groups to few of the senses - \n",
    "    - Table#2 - Data table.  \n",
    "    - Table#3 - Table of contents.\n",
    "    - table#4 - Hotel/Furniture.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of the senses:\n",
      "[('disk#1', 1.0), ('disk#2', 1.0), ('disk#3', 1.0), ('disk#4', 1.0), ('disk#5', 1.0)]\n",
      "\n",
      "\n",
      "Disk#1\n",
      "====================\n",
      "Flexible_Disk#2 0.924806\n",
      "Rigid_Disk#2 0.916348\n",
      "Basic_Input#1 0.899887\n",
      "RDOS#1 0.893633\n",
      "SASI#1 0.891424\n",
      "Sasi#1 0.891424\n",
      "corrector#4 0.889009\n",
      "customizer#2 0.888149\n",
      "Fargo#3 0.886290\n",
      "fargo#2 0.886290\n",
      "\n",
      "\n",
      "Disk#2\n",
      "====================\n",
      "DISK#2 0.991812\n",
      "BOOT#4 0.958993\n",
      "Bit#4 0.934779\n",
      "disk#2 0.931051\n",
      "module#3 0.923883\n",
      "Module#3 0.921607\n",
      "MODULE#3 0.916528\n",
      "Disks#3 0.909954\n",
      "rom#1 0.903984\n",
      "hard_disk#1 0.903680\n",
      "\n",
      "\n",
      "Disk#3\n",
      "====================\n",
      "disk#3 1.000000\n",
      "DISK#3 1.000000\n",
      "urn#4 0.960472\n",
      "URN#4 0.960472\n",
      "Urn#4 0.960472\n",
      "EXEC#4 0.958112\n",
      "Coco#4 0.957627\n",
      "CoCo#2 0.957627\n",
      "coco#3 0.957627\n",
      "COCO#2 0.957627\n",
      "\n",
      "\n",
      "Disk#4\n",
      "====================\n",
      "DISK#4 1.000000\n",
      "disk#4 0.991975\n",
      "Envelope#1 0.931414\n",
      "Plate#1 0.922727\n",
      "Stack#3 0.906151\n",
      "STACK#3 0.904850\n",
      "Shelf#4 0.900630\n",
      "stack#3 0.897358\n",
      "Bubble#4 0.896194\n",
      "CHAIN#4 0.895439\n",
      "\n",
      "\n",
      "disk#1\n",
      "====================\n",
      "DISK#1 0.995354\n",
      "Modem#2 0.961432\n",
      "install#3 0.960586\n",
      "SYNC#1 0.960527\n",
      "sync#1 0.960527\n",
      "Install#1 0.959036\n",
      "Handheld#2 0.958145\n",
      "laptop#1 0.956479\n",
      "Laptop#1 0.956479\n",
      "adapter#3 0.955576\n",
      "\n",
      "\n",
      "disk#2\n",
      "====================\n",
      "Hard_Disk#1 0.951037\n",
      "hard_disk#1 0.951037\n",
      "Disk_Drive#1 0.949592\n",
      "disk_drive#1 0.949592\n",
      "Disks#3 0.948765\n",
      "USB_flash#1 0.947548\n",
      "RAM_disk#2 0.947467\n",
      "Floppy#1 0.946784\n",
      "RAID_array#1 0.946609\n",
      "Floppy_disk#1 0.943072\n",
      "\n",
      "\n",
      "disk#3\n",
      "====================\n",
      "Disk#3 1.000000\n",
      "DISK#3 1.000000\n",
      "urn#4 0.960472\n",
      "URN#4 0.960472\n",
      "Urn#4 0.960472\n",
      "EXEC#4 0.958112\n",
      "Coco#4 0.957627\n",
      "CoCo#2 0.957627\n",
      "coco#3 0.957627\n",
      "COCO#2 0.957627\n",
      "\n",
      "\n",
      "disk#4\n",
      "====================\n",
      "Disk#4 0.991975\n",
      "DISK#4 0.991975\n",
      "Envelope#1 0.919594\n",
      "Plate#1 0.909810\n",
      "Diaphragm#1 0.896887\n",
      "diaphragm#1 0.892381\n",
      "Bulb#3 0.891742\n",
      "CHAIN#4 0.890951\n",
      "Stack#3 0.890267\n",
      "STACK#3 0.889386\n",
      "\n",
      "\n",
      "disk#5\n",
      "====================\n",
      "gaseous_envelope#1 0.940637\n",
      "Protostar#2 0.939690\n",
      "protostar#2 0.939690\n",
      "circumstellar_disk#1 0.933108\n",
      "accretion_disk#1 0.931165\n",
      "ergosphere#2 0.930390\n",
      "protoplanetary_disc#1 0.930362\n",
      "accretion_disc#2 0.929459\n",
      "protoplanetary_disk#1 0.928282\n",
      "spheroid#4 0.926040\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_word = \"disk\"\n",
    "\n",
    "print(\"Probabilities of the senses:\\n{}\\n\\n\".format(sense_vectors.get_senses(test_word, ignore_case=False)))\n",
    "\n",
    "for sense_id, prob in sense_vectors.get_senses(test_word, ignore_case=True):\n",
    "    print(sense_id)\n",
    "    print(\"=\"*20)\n",
    "    for rsense_id, sim in sense_vectors.wv.most_similar(sense_id):\n",
    "        print(\"{} {:f}\".format(rsense_id, sim))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get disambiguated sense of the word, using corpus as context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input\n",
    "To understand the word's sense in a given context, we use the *WSD* class from the sensegram library.  \n",
    "The WSD model takes the following key parameters to decide word sense based on corpus context - \n",
    "* vectors - Both sense and word vector models loaded earlier.  \n",
    "  \n",
    "  \n",
    "* method - To calculate the sense of the word, the library averages the sense scores of all the surrounding context words and compares it with different senses of the target word. For this comparison, there are two available metrics - \n",
    " - sim: Uses cosine distance\n",
    " - prob: Use log probability score  \n",
    "  \n",
    "  \n",
    "* window - This is the window(±) that the model looks into, to decide the word context.   \n",
    "For example, if our target word is *table*,   \n",
    "with the context of *\"we load the our data into a data-frame table object and count the number of rows/columns using the .shape method\"*  \n",
    " 1. a window of 3 would consider the following 6(3 on the left, and 3 on the right) words around our context word to find the sense of the word - *into, a, data-frame, object, and, count*  \n",
    " 2. a window of 5 would use the following context - *our, data, into, a, data-frame, table, object, and, count, the, number*  \n",
    "  \n",
    "  \n",
    "* verbose - Allows to print intermediate outputs while running the disambiguation code\n",
    "\n",
    "<hr>  \n",
    "     \n",
    "Some food-for-thought regarding the usage of WSD module - \n",
    " - Do note that while stopwords like *and* and *the* are considered in the context of the the target word, they are dropped while disambiguating the sense of our target.\n",
    " - While it may seem ideal to choose a high value of window for getting the sense of the target word, it may happen that the wider window results in an less accurate output, as it averages across all possible senses.\n",
    " - The library considers, and disambiguates, only the first occurance of the target word in the context. For a large corpus, it would be ideal to first split the corpus and generate contexts using an external helper function, and then iteratively get the sense for the target word across all occurances in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd_model = WSD(sense_vectors, word_vectors, window=15, method='prob', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted context words:\n",
      "['know', 'hare', 'street', 'hope', 'overexcited', 'dont', 'er', 'richard', 'still', 'aaw', 'say', 'welcome', 'open', 'poppet', 'closed', 'gif', 'tortoise', 'youre', 'got']\n",
      "Senses of a target word:\n",
      "[('pizza#1', 1.0), ('pizza#2', 1.0)]\n",
      "Significance scores of context words:\n",
      "[0.29148820525520214, 0.12975690243744742, 0.02722264833952448, 0.11942657176932528, 0.009472310267542472, 0.16188559353799392, 0.1438541919856201, 0.13226385889523357, 0.33827481693934414, 0.01728681152264855, 0.3557976824709924, 0.06579240374490092, 0.11683631808681638, 0.07629630608727012, 0.4041725965133941, 0.025504092740760764, 0.16010984464621203, 0.04979557551486591, 0.34149093466603153]\n",
      "Context words:\n",
      "closed\t0.404\n",
      "say\t0.356\n",
      "got\t0.341\n",
      "('pizza#2', [0.7334760438909177, 0.7729272994918818])\n"
     ]
    }
   ],
   "source": [
    "print(wsd_model.disambiguate(corpus_data, \"pizza\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Output </h5>  \n",
    "\n",
    "Running the Sense disambiguation code generates following lines of output -  \n",
    "1. Prints the context words extracted from the corpus.\n",
    "- Prints possible senses of the word, with their respective probabilities(without considering the context)\n",
    "- Prints the significance score of each context word.\n",
    "- Prints the most significant context words.\n",
    "- **Returns** a tuple of the sense of the word as derived from the context, and match scores(log-probability or cosine-similarity depending on the *method* chosen) of various senses of the target word.  \n",
    "For instance, the output *('table#2', [0.2706353009709064, 0.9591583572384959, 0.40617065436041355, 0.6940131864117054])* indicates the following things regarding our target word -  \n",
    "    - The closest sense of our target word is with *table#2*, with a match score of 0.959(second in the list)\n",
    "    - For the other senses, the match score can be read as follows - \n",
    "    - table#1 - 0.2706\n",
    "    - table#2 - 0.959\n",
    "    - table#3 - 0.406\n",
    "    - table#4 - 0.694\n",
    "\n",
    "Since we had not defined the *ignore_case* argument while initializing the WSD model, it resorts to the default of True, and the output return scores for the 4 senses of the word *table*.  \n",
    "If we chose to ignore case, the output would have match for 8 senses(4-Table; 4-table)\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24929"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data.find(\"pizza\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate sense embeddings from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_data_with_context(corpus_filepath, window=5, force_update=False):\n",
    "    nlp = spacy.load('en')\n",
    "    with open(corpus_fpath, \"r\") as f:\n",
    "        corpus_data = f.readlines()\n",
    "    out_file = os.path.join(os.path.dirname(corpus_fpath), f\"window_{window}_context_corpus.txt\")\n",
    "    contextual_data = []\n",
    "    if os.path.exists(out_file) and (not force_update):\n",
    "        print(\"Found preexisting file. Loading context data from file\")\n",
    "        with open(out_file, \"r\") as f:\n",
    "            contextual_data = f.readlines()\n",
    "    else:\n",
    "        print(\"No pre created corpus found, or force update flag is true. Generating contextual data from corpus\")\n",
    "        with open(out_file, \"w\") as f:\n",
    "            for txt_line in corpus_data:\n",
    "                nlp_data = nlp(txt_line.replace(\"\\n\", \"\"))\n",
    "                max_tokens = len(nlp_data)\n",
    "                for i, tok in enumerate(nlp_data):\n",
    "                    if \"NN\" in tok.tag_:\n",
    "                        start = max(0, i-window)\n",
    "                        end = min(i+window, max_tokens)\n",
    "                        left_context = [t.text for t in nlp_data[start:i]] + [f\"<{tok.text}>\"]\n",
    "                        right_context = [t.text for t in nlp_data[i+1:end]]\n",
    "                        noun_in_context = f\"<{tok.text}> - {' '.join(left_context + right_context)}\"\n",
    "                        contextual_data.append(noun_in_context)\n",
    "                        f.write(noun_in_context + \"\\n\")\n",
    "    return contextual_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_group_from_corpus(context_data, wsd_model, sense_vectors, output_file):\n",
    "    output = {\n",
    "        \"word\": [],\n",
    "        \"context\": [],\n",
    "        \"sense_id\": [],\n",
    "        \"sense_group_name\": [],\n",
    "        \"sense_group_num\": [],\n",
    "        \"sense_probability\": [],\n",
    "        \"related_senses\": []\n",
    "    }\n",
    "    \n",
    "    for row in tqdm(context_data, total=len(context_data)):\n",
    "        word, ctx = row.split(' - ')\n",
    "        sense_id, sense_probs = wsd_model.disambiguate(ctx, word)\n",
    "        sense_probability = max(sense_probs)\n",
    "        sense_group_name, sense_group_num = sense_id.split(\"#\")\n",
    "        try:\n",
    "            related_senses = [r_senseid for r_senseid,_ in sense_vectors.wv.most_similar(sense_id)]\n",
    "            related_senses_l2 = [r_senseid for related_sense in related_senses for r_senseid,_ in sense_vectors.wv.most_similar(related_sense)]\n",
    "        except KeyError as e:\n",
    "            print(f\"Could not get related senses for {sense_id}\")\n",
    "            related_senses = []\n",
    "            related_senses_l2 = []\n",
    "        output[\"word\"].append(word)\n",
    "        output[\"context\"].append(ctx)\n",
    "        output[\"sense_id\"].append(sense_id)\n",
    "        output[\"sense_group_name\"].append(sense_group_name)\n",
    "        output[\"sense_group_num\"].append(sense_group_num)\n",
    "        output[\"sense_probability\"].append(sense_probability)\n",
    "        output[\"related_senses\"].append(related_senses+related_senses_l2)\n",
    "    \n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 15\n",
    "corpus_data_sense_mappings_file = os.path.join(data_directory, \"corpus_sense_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(corpus_data_sense_mappings_file):\n",
    "    wsd_model2 = WSD(sense_vectors, word_vectors, window=window, method='prob', verbose=False)\n",
    "    contextual_data = prepare_corpus_data_with_context(corpus_fpath, window=window, force_update=False)\n",
    "    sense_group_dataframe = get_sense_group_from_corpus(contextual_data, wsd_model2, sense_vectors, corpus_data_sense_mappings_file)\n",
    "else:\n",
    "    sense_group_dataframe = pd.read_csv(corpus_data_sense_mappings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>context</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>sense_group_name</th>\n",
       "      <th>sense_group_num</th>\n",
       "      <th>sense_probability</th>\n",
       "      <th>related_senses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;english&gt;</td>\n",
       "      <td>can not stop listening to what have the &lt;engli...</td>\n",
       "      <td>english#6</td>\n",
       "      <td>english</td>\n",
       "      <td>6</td>\n",
       "      <td>0.962645</td>\n",
       "      <td>['English#6', 'ENGLISH#6', 'bengali#1', 'Sinha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;saturday&gt;</td>\n",
       "      <td>can not stop listening to what have the englis...</td>\n",
       "      <td>saturday#1</td>\n",
       "      <td>saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985751</td>\n",
       "      <td>['Saturday#2', 'SATURDAY#2', 'SUNDAY#3', 'sund...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bit&gt;</td>\n",
       "      <td>not stop listening to what have the english by...</td>\n",
       "      <td>bit#6</td>\n",
       "      <td>bit</td>\n",
       "      <td>6</td>\n",
       "      <td>0.987740</td>\n",
       "      <td>['Bit#5', 'BIT#5', 'stuff#3', 'Stuff#2', 'Real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;one&gt;</td>\n",
       "      <td>the &lt;one&gt; there is amazing had such a laugh wh...</td>\n",
       "      <td>one#3</td>\n",
       "      <td>one</td>\n",
       "      <td>3</td>\n",
       "      <td>0.969355</td>\n",
       "      <td>['One#3', 'ONE#3', 'List#10', 'Top#8', 'TOP#9'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;laugh&gt;</td>\n",
       "      <td>the one there is amazing had such a &lt;laugh&gt; wh...</td>\n",
       "      <td>laugh#1</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>['LAUGH#1', 'Laugh#2', 'giggle#1', 'Giggle#1',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word                                            context    sense_id  \\\n",
       "0   <english>  can not stop listening to what have the <engli...   english#6   \n",
       "1  <saturday>  can not stop listening to what have the englis...  saturday#1   \n",
       "2       <bit>  not stop listening to what have the english by...       bit#6   \n",
       "3       <one>  the <one> there is amazing had such a laugh wh...       one#3   \n",
       "4     <laugh>  the one there is amazing had such a <laugh> wh...     laugh#1   \n",
       "\n",
       "  sense_group_name  sense_group_num  sense_probability  \\\n",
       "0          english                6           0.962645   \n",
       "1         saturday                1           0.985751   \n",
       "2              bit                6           0.987740   \n",
       "3              one                3           0.969355   \n",
       "4            laugh                1           0.914474   \n",
       "\n",
       "                                      related_senses  \n",
       "0  ['English#6', 'ENGLISH#6', 'bengali#1', 'Sinha...  \n",
       "1  ['Saturday#2', 'SATURDAY#2', 'SUNDAY#3', 'sund...  \n",
       "2  ['Bit#5', 'BIT#5', 'stuff#3', 'Stuff#2', 'Real...  \n",
       "3  ['One#3', 'ONE#3', 'List#10', 'Top#8', 'TOP#9'...  \n",
       "4  ['LAUGH#1', 'Laugh#2', 'giggle#1', 'Giggle#1',...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_group_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypernymy extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_fpath, \"r\") as f:\n",
    "    corpus_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"direct_match\": 8,\n",
    "    \"direct_nomatch\": 5,\n",
    "    \"l1_match\": 3,\n",
    "    \"l1_nomatch\": 2,\n",
    "    \"l2_match\": 1,\n",
    "    \"l2_nomatch\": 0.5,\n",
    "}\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(WordnetAnnotator(nlp.lang), after='tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypernymy(word, idx, hypernymy_dict, corpus, level=\"l1\"):\n",
    "    token = nlp(str(word))[0]\n",
    "    synsets = token._.wordnet.synsets()\n",
    "    for syn in synsets:\n",
    "        if syn.name().split(\".n.\")[0] == word and int(syn.name().split(\".n.\")[1]) == idx:\n",
    "            match_str = \"match\"\n",
    "        else:\n",
    "            match_str = \"nomatch\"\n",
    "\n",
    "        hypernym_syn = syn.hypernyms()\n",
    "        hyponym_syn = syn.hyponyms()\n",
    "        meronym_syn = syn.part_meronyms()\n",
    "        if len(hypernym_syn) >0:\n",
    "            hypernym = hypernym_syn[0].name().split('.')[0]\n",
    "            rev_map = {\n",
    "                f\"{word}#{idx}\":syn,\n",
    "                \"hyponyms\": [hyp.name().split(\".\")[0] for hyp in hyponym_syn if hyp.name().split(\".\")[0]],\n",
    "                \"meronyms\": [mero.name().split(\".\")[0] for mero in meronym_syn if mero.name().split(\".\")[0]]\n",
    "            }\n",
    "            if level in hypernymy_dict[hypernym][\"rev_map\"]:\n",
    "                hypernymy_dict[hypernym][\"rev_map\"][level].append(rev_map) \n",
    "            else:\n",
    "                hypernymy_dict[hypernym][\"rev_map\"][level] = [rev_map]\n",
    "\n",
    "            if \"sum_weight\" in hypernymy_dict[hypernym]:\n",
    "                hypernymy_dict[hypernym][\"sum_weight\"] += weights[f\"{level}_{match_str}\"]\n",
    "            else:\n",
    "                hypernymy_dict[hypernym][\"sum_weight\"] = weights[f\"{level}_{match_str}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyponyms_meronym(hypernymy_dict, hypernym, kind=\"hyponyms\"):\n",
    "    nym = []\n",
    "    reverse_map = hypernymy_dict[hypernym]['rev_map']\n",
    "    extract_from = []\n",
    "    if \"direct\" in reverse_map:\n",
    "        extract_from.append(\"direct\")\n",
    "    if \"l1\" in reverse_map:\n",
    "        extract_from.append(\"l1\")\n",
    "    \n",
    "    for relation in extract_from:\n",
    "        num_relations = len(reverse_map[relation])\n",
    "        for i in range(num_relations):\n",
    "            nym += reverse_map[relation][i][kind]\n",
    "            \n",
    "    return list(set(nym))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1351/1351 [21:46<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "all_hypernyms = []\n",
    "all_hhm_map = []\n",
    "all_hyponyms = []\n",
    "all_meronyms = []\n",
    "\n",
    "all_hypernyms_from_corpus = []\n",
    "all_hyponyms_from_corpus = []\n",
    "all_meronyms_from_corpus = []\n",
    "num_words = len(sense_group_dataframe)\n",
    "for _, row in tqdm(sense_group_dataframe[[\"sense_group_name\", \"sense_group_num\", \"related_senses\"]].iterrows(), total=num_words):\n",
    "    word, idx, r_senses = row\n",
    "    \n",
    "    hypernymy_dict = defaultdict(lambda: defaultdict(dict))\n",
    "    generate_hypernymy(word, idx, hypernymy_dict, corpus, level=\"direct\")\n",
    "    r_senses_list = ast.literal_eval(r_senses)\n",
    "    for w in r_senses_list:\n",
    "        related_word, related_idx = str(w).split(\"#\")\n",
    "        if related_word in corpus:\n",
    "            generate_hypernymy(related_word, related_idx, corpus, hypernymy_dict)\n",
    "    if len(hypernymy_dict)>0:\n",
    "        hypernymy_dict_sorted = {k: v for k, v in sorted(hypernymy_dict.items(), key=lambda item: item[1]['sum_weight'], reverse=True)}\n",
    "        \n",
    "        possible_hypernyms = list(hypernymy_dict_sorted.keys())\n",
    "        hypernym_name = possible_hypernyms[0]\n",
    "        hyponyms = extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym_name, kind=\"hyponyms\")\n",
    "        meronyms = extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym_name, kind=\"meronyms\")\n",
    "        \n",
    "        all_hypernyms.append(hypernym_name)\n",
    "        all_hyponyms.append(hyponyms)\n",
    "        all_meronyms.append(meronyms)\n",
    "        \n",
    "        \n",
    "        hypernym_from_corpus = None\n",
    "        hyponyms_from_corpus = []\n",
    "        meronyms_from_corpus = []\n",
    "        for hypernym in possible_hypernyms:\n",
    "            if hypernym in corpus_data:\n",
    "                hypernym_from_corpus = hypernym\n",
    "        if hypernym_from_corpus:\n",
    "            hyponyms += extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym_from_corpus, kind=\"hyponyms\")\n",
    "            meronyms += extract_hyponyms_meronym(hypernymy_dict_sorted, hypernym_from_corpus, kind=\"meronyms\")\n",
    "            \n",
    "        for hypo in hyponyms:\n",
    "            if hypo in corpus_data:\n",
    "                hyponyms_from_corpus.append(hypo)\n",
    "                \n",
    "        for mero in meronyms:\n",
    "            if mero in corpus_data:\n",
    "                meronyms_from_corpus.append(mero)\n",
    "        \n",
    "        \n",
    "        all_hypernyms_from_corpus.append(hypernym_from_corpus)\n",
    "        all_hyponyms_from_corpus.append(hyponyms_from_corpus)\n",
    "        all_meronyms_from_corpus.append(meronyms_from_corpus)\n",
    "        all_hhm_map.append(hypernymy_dict_sorted)\n",
    "    else:\n",
    "        all_hypernyms.append(None)\n",
    "        all_hyponyms.append([])\n",
    "        all_meronyms.append([])\n",
    "        all_hypernyms_from_corpus.append([])\n",
    "        all_hyponyms_from_corpus.append([])\n",
    "        all_meronyms_from_corpus.append([])\n",
    "        all_hhm_map.append(hypernymy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_group_dataframe[\"hhm_map\"] = all_hhm_map\n",
    "sense_group_dataframe[\"hypernym\"] = all_hypernyms\n",
    "sense_group_dataframe[\"hyponym\"] = all_hyponyms\n",
    "sense_group_dataframe[\"meronym\"] = all_meronyms\n",
    "sense_group_dataframe[\"hypernym_from_corpus\"] = all_hypernyms_from_corpus\n",
    "sense_group_dataframe[\"hyponym_from_corpus\"] = all_hyponyms_from_corpus\n",
    "sense_group_dataframe[\"meronym_from_corpus\"] = all_meronyms_from_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hhm_mappings_file = os.path.join(data_directory, \"hhm_mappings.csv\")\n",
    "sense_group_dataframe.to_csv(hhm_mappings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>context</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>sense_group_name</th>\n",
       "      <th>sense_group_num</th>\n",
       "      <th>sense_probability</th>\n",
       "      <th>related_senses</th>\n",
       "      <th>hhm_map</th>\n",
       "      <th>hypernym</th>\n",
       "      <th>hyponym</th>\n",
       "      <th>meronym</th>\n",
       "      <th>hypernym_from_corpus</th>\n",
       "      <th>hyponym_from_corpus</th>\n",
       "      <th>meronym_from_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;english&gt;</td>\n",
       "      <td>can not stop listening to what have the &lt;engli...</td>\n",
       "      <td>english#6</td>\n",
       "      <td>english</td>\n",
       "      <td>6</td>\n",
       "      <td>0.962645</td>\n",
       "      <td>['English#6', 'ENGLISH#6', 'bengali#1', 'Sinha...</td>\n",
       "      <td>{'sanskrit': {'rev_map': {'l1': [{'Sinhala#3':...</td>\n",
       "      <td>sanskrit</td>\n",
       "      <td>[hindustani]</td>\n",
       "      <td>[]</td>\n",
       "      <td>indic</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;saturday&gt;</td>\n",
       "      <td>can not stop listening to what have the englis...</td>\n",
       "      <td>saturday#1</td>\n",
       "      <td>saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>0.985751</td>\n",
       "      <td>['Saturday#2', 'SATURDAY#2', 'SUNDAY#3', 'sund...</td>\n",
       "      <td>{'weekday': {'rev_map': {'direct': [{'saturday...</td>\n",
       "      <td>weekday</td>\n",
       "      <td>[whit-tuesday, whitmonday]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;bit&gt;</td>\n",
       "      <td>not stop listening to what have the english by...</td>\n",
       "      <td>bit#6</td>\n",
       "      <td>bit</td>\n",
       "      <td>6</td>\n",
       "      <td>0.987740</td>\n",
       "      <td>['Bit#5', 'BIT#5', 'stuff#3', 'Stuff#2', 'Real...</td>\n",
       "      <td>{'fragment': {'rev_map': {'direct': [{'bit#6':...</td>\n",
       "      <td>fragment</td>\n",
       "      <td>[matchwood, scale, scurf, splinter, admit]</td>\n",
       "      <td>[]</td>\n",
       "      <td>spend</td>\n",
       "      <td>[admit]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;one&gt;</td>\n",
       "      <td>the &lt;one&gt; there is amazing had such a laugh wh...</td>\n",
       "      <td>one#3</td>\n",
       "      <td>one</td>\n",
       "      <td>3</td>\n",
       "      <td>0.969355</td>\n",
       "      <td>['One#3', 'ONE#3', 'List#10', 'Top#8', 'TOP#9'...</td>\n",
       "      <td>{'be': {'rev_map': {'l1': [{'remaining#2': Syn...</td>\n",
       "      <td>be</td>\n",
       "      <td>[keep_out, sit_tight, stick_together, bide, ou...</td>\n",
       "      <td>[]</td>\n",
       "      <td>name</td>\n",
       "      <td>[be, stand, make]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;laugh&gt;</td>\n",
       "      <td>the one there is amazing had such a &lt;laugh&gt; wh...</td>\n",
       "      <td>laugh#1</td>\n",
       "      <td>laugh</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>['LAUGH#1', 'Laugh#2', 'giggle#1', 'Giggle#1',...</td>\n",
       "      <td>{'laugh': {'rev_map': {'l1': [{'giggle#1': Syn...</td>\n",
       "      <td>laugh</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>moment</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>&lt;duck&gt;</td>\n",
       "      <td>you lucky &lt;duck&gt;\\n</td>\n",
       "      <td>duck#2</td>\n",
       "      <td>duck</td>\n",
       "      <td>2</td>\n",
       "      <td>0.869252</td>\n",
       "      <td>['toucan#2', 'Toucan#2', 'frog#2', 'FROG#2', '...</td>\n",
       "      <td>{'have': {'rev_map': {'l1': [{'bear#1': Synset...</td>\n",
       "      <td>have</td>\n",
       "      <td>[carry]</td>\n",
       "      <td>[]</td>\n",
       "      <td>persuade</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>&lt;slices&gt;</td>\n",
       "      <td>not really I just had two &lt;slices&gt; of leftover...</td>\n",
       "      <td>slices#1</td>\n",
       "      <td>slices</td>\n",
       "      <td>1</td>\n",
       "      <td>0.918132</td>\n",
       "      <td>['Slices#1', 'bite_sized#1', 'fillets#1', 'Jul...</td>\n",
       "      <td>{'cook': {'rev_map': {'l1': [{'fry#5': Synset(...</td>\n",
       "      <td>cook</td>\n",
       "      <td>[deep-fat-fry, pan-fry, frizzle, saute, stir_f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>walk</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>&lt;leftover&gt;</td>\n",
       "      <td>not really I just had two slices of &lt;leftover&gt;...</td>\n",
       "      <td>leftover#1</td>\n",
       "      <td>leftover</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936025</td>\n",
       "      <td>['Leftover#1', 'Scraps#1', 'scraps#1', 'scaven...</td>\n",
       "      <td>{'waste': {'rev_map': {'l1': [{'Scraps#1': Syn...</td>\n",
       "      <td>waste</td>\n",
       "      <td>[litter, debris, scrap_metal]</td>\n",
       "      <td>[]</td>\n",
       "      <td>mass</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>&lt;pizza&gt;</td>\n",
       "      <td>not really I just had two slices of leftover h...</td>\n",
       "      <td>pizza#2</td>\n",
       "      <td>pizza</td>\n",
       "      <td>2</td>\n",
       "      <td>0.885774</td>\n",
       "      <td>['hamburger#1', 'Chicken_Sandwich#1', 'chicken...</td>\n",
       "      <td>{'sandwich': {'rev_map': {'l1': [{'hamburger#1...</td>\n",
       "      <td>sandwich</td>\n",
       "      <td>[chili_dog, cheeseburger]</td>\n",
       "      <td>[ground_beef, frankfurter_bun, frank]</td>\n",
       "      <td>potato</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>&lt;today&gt;</td>\n",
       "      <td>not really I just had two slices of leftover h...</td>\n",
       "      <td>today#6</td>\n",
       "      <td>today</td>\n",
       "      <td>6</td>\n",
       "      <td>0.951567</td>\n",
       "      <td>['modern#3', 'present#3', 'Present#3', 'olden#...</td>\n",
       "      <td>{'present': {'rev_map': {'direct': [{'today#6'...</td>\n",
       "      <td>present</td>\n",
       "      <td>[amusement_park, village_green]</td>\n",
       "      <td>[]</td>\n",
       "      <td>tract</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1351 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word                                            context  \\\n",
       "0      <english>  can not stop listening to what have the <engli...   \n",
       "1     <saturday>  can not stop listening to what have the englis...   \n",
       "2          <bit>  not stop listening to what have the english by...   \n",
       "3          <one>  the <one> there is amazing had such a laugh wh...   \n",
       "4        <laugh>  the one there is amazing had such a <laugh> wh...   \n",
       "...          ...                                                ...   \n",
       "1346      <duck>                                 you lucky <duck>\\n   \n",
       "1347    <slices>  not really I just had two <slices> of leftover...   \n",
       "1348  <leftover>  not really I just had two slices of <leftover>...   \n",
       "1349     <pizza>  not really I just had two slices of leftover h...   \n",
       "1350     <today>  not really I just had two slices of leftover h...   \n",
       "\n",
       "        sense_id sense_group_name  sense_group_num  sense_probability  \\\n",
       "0      english#6          english                6           0.962645   \n",
       "1     saturday#1         saturday                1           0.985751   \n",
       "2          bit#6              bit                6           0.987740   \n",
       "3          one#3              one                3           0.969355   \n",
       "4        laugh#1            laugh                1           0.914474   \n",
       "...          ...              ...              ...                ...   \n",
       "1346      duck#2             duck                2           0.869252   \n",
       "1347    slices#1           slices                1           0.918132   \n",
       "1348  leftover#1         leftover                1           0.936025   \n",
       "1349     pizza#2            pizza                2           0.885774   \n",
       "1350     today#6            today                6           0.951567   \n",
       "\n",
       "                                         related_senses  \\\n",
       "0     ['English#6', 'ENGLISH#6', 'bengali#1', 'Sinha...   \n",
       "1     ['Saturday#2', 'SATURDAY#2', 'SUNDAY#3', 'sund...   \n",
       "2     ['Bit#5', 'BIT#5', 'stuff#3', 'Stuff#2', 'Real...   \n",
       "3     ['One#3', 'ONE#3', 'List#10', 'Top#8', 'TOP#9'...   \n",
       "4     ['LAUGH#1', 'Laugh#2', 'giggle#1', 'Giggle#1',...   \n",
       "...                                                 ...   \n",
       "1346  ['toucan#2', 'Toucan#2', 'frog#2', 'FROG#2', '...   \n",
       "1347  ['Slices#1', 'bite_sized#1', 'fillets#1', 'Jul...   \n",
       "1348  ['Leftover#1', 'Scraps#1', 'scraps#1', 'scaven...   \n",
       "1349  ['hamburger#1', 'Chicken_Sandwich#1', 'chicken...   \n",
       "1350  ['modern#3', 'present#3', 'Present#3', 'olden#...   \n",
       "\n",
       "                                                hhm_map  hypernym  \\\n",
       "0     {'sanskrit': {'rev_map': {'l1': [{'Sinhala#3':...  sanskrit   \n",
       "1     {'weekday': {'rev_map': {'direct': [{'saturday...   weekday   \n",
       "2     {'fragment': {'rev_map': {'direct': [{'bit#6':...  fragment   \n",
       "3     {'be': {'rev_map': {'l1': [{'remaining#2': Syn...        be   \n",
       "4     {'laugh': {'rev_map': {'l1': [{'giggle#1': Syn...     laugh   \n",
       "...                                                 ...       ...   \n",
       "1346  {'have': {'rev_map': {'l1': [{'bear#1': Synset...      have   \n",
       "1347  {'cook': {'rev_map': {'l1': [{'fry#5': Synset(...      cook   \n",
       "1348  {'waste': {'rev_map': {'l1': [{'Scraps#1': Syn...     waste   \n",
       "1349  {'sandwich': {'rev_map': {'l1': [{'hamburger#1...  sandwich   \n",
       "1350  {'present': {'rev_map': {'direct': [{'today#6'...   present   \n",
       "\n",
       "                                                hyponym  \\\n",
       "0                                          [hindustani]   \n",
       "1                            [whit-tuesday, whitmonday]   \n",
       "2            [matchwood, scale, scurf, splinter, admit]   \n",
       "3     [keep_out, sit_tight, stick_together, bide, ou...   \n",
       "4                                                    []   \n",
       "...                                                 ...   \n",
       "1346                                            [carry]   \n",
       "1347  [deep-fat-fry, pan-fry, frizzle, saute, stir_f...   \n",
       "1348                      [litter, debris, scrap_metal]   \n",
       "1349                          [chili_dog, cheeseburger]   \n",
       "1350                    [amusement_park, village_green]   \n",
       "\n",
       "                                    meronym hypernym_from_corpus  \\\n",
       "0                                        []                indic   \n",
       "1                                        []                 None   \n",
       "2                                        []                spend   \n",
       "3                                        []                 name   \n",
       "4                                        []               moment   \n",
       "...                                     ...                  ...   \n",
       "1346                                     []             persuade   \n",
       "1347                                     []                 walk   \n",
       "1348                                     []                 mass   \n",
       "1349  [ground_beef, frankfurter_bun, frank]               potato   \n",
       "1350                                     []                tract   \n",
       "\n",
       "     hyponym_from_corpus meronym_from_corpus  \n",
       "0                     []                  []  \n",
       "1                     []                  []  \n",
       "2                [admit]                  []  \n",
       "3      [be, stand, make]                  []  \n",
       "4                     []                  []  \n",
       "...                  ...                 ...  \n",
       "1346                  []                  []  \n",
       "1347                  []                  []  \n",
       "1348                  []                  []  \n",
       "1349                  []                  []  \n",
       "1350                  []                  []  \n",
       "\n",
       "[1351 rows x 14 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_group_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
